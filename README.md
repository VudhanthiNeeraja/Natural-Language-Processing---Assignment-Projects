# Natural-Language-Processing---Assignment-Projects
These assignment projects include a series of reports and corresponding Python scripts that document and implement a comprehensive NLP workflow. <br> 
<br>
### __Project-Assignment 1:__ <br>
This assignment involves training Word2Vec models using Gensim to generate word embeddings and comparing their performance using various tasks. <br>
<br>
__Task 1:__ Train Word2Vec Models <br>
Train Word2Vec on a selected corpus using Gensim in two different ways to create two sets of word embeddings. The variations could include using SkipGram versus CBOW, varying the number of epochs, changing the amount of data used, using different corpora, applying different preprocessing steps (such as lowercasing or lemmatizing words), or altering the vector lengths. Save the resulting word embeddings in plain text .txt files. <br>
<br>
__Task 2:__ Visualization of Embeddings <br>
Create a custom set of 20 words and visualize their embeddings using the two sets of embeddings from Task 1. Utilize the provided A1_helper.py module for visualization. <br>
<br>
__Task 3:__ Similarity Evaluation <br>
Create a dataset of at least 10 word pairs with similarity scores ranging from 0 to 1, based on your judgment. Ensure all words have embeddings. Evaluate the two sets of embeddings from Task 1 and the pre-trained Google News embeddings on this dataset using the Pearson correlation coefficient. Save the dataset in a tab-delimited .txt file. <br>
<br>
__Task 4:__ Finding Most Similar Words <br>
Select any 5 words (which can be from the previous steps) and find their five most similar words using the three sets of word embeddings (the two trained by you and the pre-trained Google News embeddings). <br>
<br>
### __Project-Assignment 2:__ <br>
This assignment involves training and evaluating Recurrent Neural Network (RNN) models for Named Entity Recognition (NER) tasks, performing error analysis, and validating the models on custom sentences. <br>
<br> 
__Task 1:__ Train and Evaluate RNN Models <br>
Train and evaluate RNN models using two different approaches to make meaningful comparisons. The comparisons could involve different types of RNN layers (e.g., LSTM vs. GRU), varying the number of nodes or layers, using bidirectional RNNs, incorporating pre-trained embeddings, or using different training set sizes while keeping the test set constant. The goal is to observe the impact of these variations on model performance. <br>
<br>
__Task 2:__ Error Analysis <br>
Conduct error analysis based on the evaluations from Task 1. Examine the eval.txt file to identify the types of mistakes made by the models and determine if the nature of these errors changes based on the different comparisons made in Task 1. Make at least five observations regarding the types of errors and their patterns. <br>
<br>
__Task 3:__ Custom Sentence Evaluation <br>
Create a dataset of 10 sentences with their correct NER tags. Store these sentences in a format that can be read and processed like te_sent and te_tag in the provided A2.py program. Evaluate the best-performing model from Task 1 on these custom sentences. <br>
<br>
### __Project-Assignment 3:__ <br>
This assignment involves fine-tuning a GPT-2 model on a specific text dataset and analyzing the differences in text generation before and after fine-tuning. Additionally, it requires identifying and discussing biases present in the language model.<br>
<br>
__Task 1:__ Fine-Tuning <br>
Model Selection and Dataset:<br>
<br>
Choose a GPT-2 model version appropriate for your computational resources. <br>
Select a text dataset in English, ensuring it is large enough (at least 1 megabyte) but not excessively large. <br>
Fine-tune the selected GPT-2 model on this dataset. <br>
Text Generation Comparison: <br>
Generate text completions for five examples using both the original GPT-2 model and the fine-tuned model. <br>
For each example, generate at least five sequences to highlight differences in the text generated by the two models. <br>
<br>
__Task 2:__ Identify Biases <br>
Bias Examples:<br>
Identify three new examples that demonstrate potential biases in the language models. <br>
Generate at least five return sequences for each example using either the original or the fine-tuned model. <br>
<br>
### __Project-Assignment 4:__ <br>
This assignment involves fine-tuning a BERT model for a classification task, analyzing the model's performance, and demonstrating BERT's capability for contextual embeddings. The tasks are detailed as follows:<br>
<br>
__Task 1:__ Fine-Tuning BERT <br>
Model and Dataset:<br>
Choose a BERT model and fine-tune it on a given training dataset. The dataset should be classified, and the final layer of the model should be a softmax layer. <br>
Select appropriate hyperparameters, including the number of epochs and batch size. <br>
Evaluate the fine-tuned model on a test dataset and report the test accuracy. <br>
<br>
__Task 2:__ Analysis of Predictions <br>
Correct and Incorrect Predictions: <br>
Examine the model's predictions on the test dataset. <br>
Identify at least 10 examples where the model's predictions are correct and 10 examples where the predictions are incorrect. <br>
From these examples, derive at least three general observations about the model's performance and behavior. <br> 
<br>
__Task 3:__ Contextual Embeddings <br>
Examples and Cosine Similarity: <br>
Create at least five examples that demonstrate BERT's contextual embeddings, similar to the "dog" and "cat" example provided in the slides. <br>
Calculate and include the cosine similarity scores for these examples. <br>
